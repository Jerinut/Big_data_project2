{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55974225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35005b8",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),  \n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),  \n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69712d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "# Load the DataFrame\n",
    ")\n",
    "# Parse JSON data from Kafka's 'value' column\n",
    "parsed_df = lines.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"parsed_value\")) \n",
    "\n",
    "df = parsed_df.select(\"parsed_value.*\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc",
   "metadata": {},
   "source": [
    "## The project starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b2d7ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`license`, `total_amount`, `timestamp`].;\n'EventTimeWatermark 'pickup_datetime, 10 minutes\n+- Project [parsed_value#54.license AS license#56, parsed_value#54.total_amount AS total_amount#57, parsed_value#54.timestamp AS timestamp#58]\n   +- Project [from_json(StructField(license,StringType,true), StructField(total_amount,DoubleType,true), StructField(timestamp,TimestampType,true), value#52, Some(Etc/UTC)) AS parsed_value#54]\n      +- Project [cast(value#39 as string) AS value#52]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1d1b055e, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6eb68be6, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092, subscribe=stock, maxOffsetsPerTrigger=100], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@12239223,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> stock, startingOffsets -> earliest, maxOffsetsPerTrigger -> 100),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m window_sizes:\n\u001b[1;32m      5\u001b[0m     window_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     utilization_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 minutes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mgroupBy(window(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, window_duration), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     11\u001b[0m             (\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m             (\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m lag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mover(Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midle_time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m         ) \\\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutilization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m coalesce(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midle_time\u001b[39m\u001b[38;5;124m\"\u001b[39m), lit(\u001b[38;5;241m0\u001b[39m))) \u001b[38;5;241m/\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Write to sink (e.g., console, file, or in-memory table for a dashboard)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     utilization_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1147\u001b[0m, in \u001b[0;36mDataFrame.withWatermark\u001b[0;34m(self, eventTime, delayThreshold)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delayThreshold \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(delayThreshold) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1141\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1142\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         },\n\u001b[1;32m   1146\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[43meventTime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayThreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`license`, `total_amount`, `timestamp`].;\n'EventTimeWatermark 'pickup_datetime, 10 minutes\n+- Project [parsed_value#54.license AS license#56, parsed_value#54.total_amount AS total_amount#57, parsed_value#54.timestamp AS timestamp#58]\n   +- Project [from_json(StructField(license,StringType,true), StructField(total_amount,DoubleType,true), StructField(timestamp,TimestampType,true), value#52, Some(Etc/UTC)) AS parsed_value#54]\n      +- Project [cast(value#39 as string) AS value#52]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1d1b055e, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6eb68be6, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092, subscribe=stock, maxOffsetsPerTrigger=100], [key#38, value#39, topic#40, partition#41, offset#42L, timestamp#43, timestampType#44], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@12239223,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> stock, startingOffsets -> earliest, maxOffsetsPerTrigger -> 100),None), kafka, [key#31, value#32, topic#33, partition#34, offset#35L, timestamp#36, timestampType#37]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_sizes = [5, 10, 15]  # Minutes\n",
    "for size in window_sizes:\n",
    "    window_duration = f\"{size} minutes\"\n",
    "\n",
    "    utilization_df = df \\\n",
    "        .withWatermark(\"pickup_datetime\", \"10 minutes\") \\\n",
    "        .groupBy(window(\"pickup_datetime\", window_duration), \"license\") \\\n",
    "        .agg(\n",
    "            (max(\"dropoff_datetime\") - min(\"pickup_datetime\")).cast(\"long\").alias(\"total_time\"),\n",
    "            (min(\"pickup_datetime\") - lag(\"dropoff_datetime\").over(Window.partitionBy(\"license\").orderBy(\"pickup_datetime\"))).cast(\"long\").alias(\"idle_time\")\n",
    "        ) \\\n",
    "        .withColumn(\"utilization\", (100 * (col(\"total_time\") - coalesce(col(\"idle_time\"), lit(0))) / col(\"total_time\")).cast(\"double\"))\n",
    "\n",
    "    # Write to sink (e.g., console, file, or in-memory table for a dashboard)\n",
    "    utilization_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime='10 seconds')\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e16795-7acd-4e3b-a28e-8d82f1ba88b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Using cached geopandas-0.14.4-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting shapely\n",
      "  Using cached shapely-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting fiona>=1.8.21 (from geopandas)\n",
      "  Using cached fiona-1.9.6-cp311-cp311-manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.11/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from geopandas) (24.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (2.0.3)\n",
      "Collecting pyproj>=3.3.0 (from geopandas)\n",
      "  Using cached pyproj-3.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (23.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (8.1.7)\n",
      "Collecting click-plugins>=1.0 (from fiona>=1.8.21->geopandas)\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting cligj>=0.5 (from fiona>=1.8.21->geopandas)\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Using cached geopandas-0.14.4-py3-none-any.whl (1.1 MB)\n",
      "Using cached shapely-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Using cached fiona-1.9.6-cp311-cp311-manylinux2014_x86_64.whl (15.7 MB)\n",
      "Using cached pyproj-3.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Installing collected packages: shapely, pyproj, cligj, click-plugins, fiona, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.6 geopandas-0.14.4 pyproj-3.6.1 shapely-2.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b62c69-d802-4742-8ba0-10db8a35367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3517: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3517: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           medallion                      hack_license  \\\n",
      "0   89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
      "1   0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
      "2   0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
      "3   DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
      "4   DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
      "5   20D9ECB2CA0767CF7A01564DF2844A3E  598CCE5B9C1918568DEE71F43CF26CD2   \n",
      "6   496644932DF3932605C22C7926FF0FE0  513189AD756FF14FE670D10B92FAF04C   \n",
      "7   0B57B9633A2FECD3D3B1944AFC7471CF  CCD4367B417ED6634D986F573A552A62   \n",
      "8   2C0E91FF20A856C891483ED63589F982  1DA2F6543A62B8ED934771661A9D2FA0   \n",
      "9   2D4B95E2FA7B2E85118EC5CA4570FA58  CD2F522EEE1FF5F5A8D8B679E23576B3   \n",
      "10  E12F6AF991172EAC3553144A0AF75A19  06918214E951FA0003D1CC54955C2AB0   \n",
      "11  E12F6AF991172EAC3553144A0AF75A19  06918214E951FA0003D1CC54955C2AB0   \n",
      "12  78FFD9CD0CDA541F335EF8B38FB494D6  E949C583ECF62C8F03FDCE1484954A08   \n",
      "13  237F49C3ECC11F5024B254268F054384  93C363DDF8ED9385D65FAD07CE3F5F07   \n",
      "14  3349F919AA8AE5DC9C50A3773EA45BD8  7CE849FEF67514F080AF80D990F7EF7F   \n",
      "15  3349F919AA8AE5DC9C50A3773EA45BD8  7CE849FEF67514F080AF80D990F7EF7F   \n",
      "16  4C005EEBAA7BF26B84B21586332488A2  351BE7D984BE17DB2FA80A748E816472   \n",
      "17  7D99C30FCE69B1A9DD27E2AEAC9BFA0C  460C3F57DD9CB2265DB75B14CD70224D   \n",
      "18  E6FBF80668FE0611AEA44FD9574A7E32  36773E80775F26CD1158EB5450A61C79   \n",
      "19  0C5296F3C8B16E702F8F2E06F5106552  D2363240A9295EF570FC6069BC4F4C92   \n",
      "\n",
      "   vendor_id  rate_code store_and_fwd_flag      pickup_datetime  \\\n",
      "0        CMT          1                  N  2013-01-01 15:11:48   \n",
      "1        CMT          1                  N  2013-01-06 00:18:35   \n",
      "2        CMT          1                  N  2013-01-05 18:49:41   \n",
      "3        CMT          1                  N  2013-01-07 23:54:15   \n",
      "4        CMT          1                  N  2013-01-07 23:25:03   \n",
      "5        CMT          1                  N  2013-01-07 15:27:48   \n",
      "6        CMT          1                  N  2013-01-08 11:01:15   \n",
      "7        CMT          1                  N  2013-01-07 12:39:18   \n",
      "8        CMT          1                  N  2013-01-07 18:15:47   \n",
      "9        CMT          1                  N  2013-01-07 15:33:28   \n",
      "10       CMT          1                  N  2013-01-08 13:11:52   \n",
      "11       CMT          1                  N  2013-01-08 09:50:05   \n",
      "12       CMT          1                  N  2013-01-10 12:07:08   \n",
      "13       CMT          1                  N  2013-01-07 07:35:47   \n",
      "14       CMT          1                  N  2013-01-10 15:42:29   \n",
      "15       CMT          1                  N  2013-01-10 14:27:28   \n",
      "16       CMT          1                  N  2013-01-07 22:09:59   \n",
      "17       CMT          1                  N  2013-01-07 17:18:16   \n",
      "18       CMT          1                  N  2013-01-07 06:08:51   \n",
      "19       CMT          1                  N  2013-01-07 22:25:46   \n",
      "\n",
      "       dropoff_datetime  passenger_count  trip_time_in_secs  trip_distance  \\\n",
      "0   2013-01-01 15:18:10                4                382            1.0   \n",
      "1   2013-01-06 00:22:54                1                259            1.5   \n",
      "2   2013-01-05 18:54:23                1                282            1.1   \n",
      "3   2013-01-07 23:58:20                2                244            0.7   \n",
      "4   2013-01-07 23:34:24                1                560            2.1   \n",
      "5   2013-01-07 15:38:37                1                648            1.7   \n",
      "6   2013-01-08 11:08:14                1                418            0.8   \n",
      "7   2013-01-07 13:10:56                3               1898           10.7   \n",
      "8   2013-01-07 18:20:47                1                299            0.8   \n",
      "9   2013-01-07 15:49:26                2                957            2.5   \n",
      "10  2013-01-08 13:19:50                1                477            1.3   \n",
      "11  2013-01-08 10:02:54                1                768            0.7   \n",
      "12  2013-01-10 12:17:29                1                620            2.3   \n",
      "13  2013-01-07 07:46:00                1                612            2.3   \n",
      "14  2013-01-10 16:04:02                1               1293            3.2   \n",
      "15  2013-01-10 14:45:21                1               1073            4.4   \n",
      "16  2013-01-07 22:19:50                1                591            1.7   \n",
      "17  2013-01-07 17:20:55                1                158            0.7   \n",
      "18  2013-01-07 06:13:14                1                262            1.7   \n",
      "19  2013-01-07 22:36:56                1                669            2.3   \n",
      "\n",
      "    pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0         -73.978165        40.757977         -73.989838         40.751171   \n",
      "1         -74.006683        40.731781         -73.994499         40.750660   \n",
      "2         -74.004707        40.737770         -74.009834         40.726002   \n",
      "3         -73.974602        40.759945         -73.984734         40.759388   \n",
      "4         -73.976250        40.748528         -74.002586         40.747868   \n",
      "5         -73.966743        40.764252         -73.983322         40.743763   \n",
      "6         -73.995804        40.743977         -74.007416         40.744343   \n",
      "7         -73.989937        40.756775         -73.865250         40.770630   \n",
      "8         -73.980072        40.743137         -73.982712         40.735336   \n",
      "9         -73.977936        40.786983         -73.952919         40.806370   \n",
      "10        -73.982452        40.773167         -73.964134         40.773815   \n",
      "11        -73.995560        40.749294         -73.988686         40.759052   \n",
      "12        -73.971497        40.791321         -73.964478         40.775921   \n",
      "13        -73.988510        40.774307         -73.981094         40.755325   \n",
      "14        -73.994911        40.723221         -73.971558         40.761612   \n",
      "15        -74.010391        40.708702         -73.987846         40.756104   \n",
      "16        -73.973732        40.756287         -73.998413         40.756832   \n",
      "17        -73.968925        40.767704         -73.961990         40.776566   \n",
      "18        -73.962120        40.769737         -73.979561         40.755390   \n",
      "19        -73.989708        40.756714         -73.977615         40.787575   \n",
      "\n",
      "   pickup_borough dropoff_borough  \n",
      "0       Manhattan       Manhattan  \n",
      "1       Manhattan       Manhattan  \n",
      "2       Manhattan       Manhattan  \n",
      "3       Manhattan       Manhattan  \n",
      "4       Manhattan       Manhattan  \n",
      "5       Manhattan       Manhattan  \n",
      "6       Manhattan       Manhattan  \n",
      "7       Manhattan          Queens  \n",
      "8       Manhattan       Manhattan  \n",
      "9       Manhattan       Manhattan  \n",
      "10      Manhattan       Manhattan  \n",
      "11      Manhattan       Manhattan  \n",
      "12      Manhattan       Manhattan  \n",
      "13      Manhattan       Manhattan  \n",
      "14      Manhattan       Manhattan  \n",
      "15      Manhattan       Manhattan  \n",
      "16      Manhattan       Manhattan  \n",
      "17      Manhattan       Manhattan  \n",
      "18      Manhattan       Manhattan  \n",
      "19      Manhattan       Manhattan  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the GeoJSON file\n",
    "boroughs_gdf = gpd.read_file('nyc-boroughs.geojson')\n",
    "\n",
    "# Load the taxi data CSV file\n",
    "taxi_df = pd.read_csv('sample.csv')\n",
    "\n",
    "# Create GeoDataFrames for pickup and dropoff locations\n",
    "pickup_gdf = gpd.GeoDataFrame(\n",
    "    taxi_df, geometry=gpd.points_from_xy(taxi_df.pickup_longitude, taxi_df.pickup_latitude))\n",
    "\n",
    "dropoff_gdf = gpd.GeoDataFrame(\n",
    "    taxi_df, geometry=gpd.points_from_xy(taxi_df.dropoff_longitude, taxi_df.dropoff_latitude))\n",
    "\n",
    "# Set the coordinate reference system (CRS) to match the GeoJSON file\n",
    "pickup_gdf.set_crs(epsg=4326, inplace=True)\n",
    "dropoff_gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Perform spatial join to find the borough for each pickup and dropoff location\n",
    "pickup_with_borough = gpd.sjoin(pickup_gdf, boroughs_gdf, how='left', op='within')\n",
    "dropoff_with_borough = gpd.sjoin(dropoff_gdf, boroughs_gdf, how='left', op='within')\n",
    "\n",
    "# Extract the borough information\n",
    "taxi_df['pickup_borough'] = pickup_with_borough['borough']\n",
    "taxi_df['dropoff_borough'] = dropoff_with_borough['borough']\n",
    "\n",
    "# Display the DataFrame with borough information\n",
    "print(taxi_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55f79ad-6b2c-4969-b8f1-3f5eb25317a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Filter trips within the last hour\u001b[39;00m\n\u001b[1;32m     49\u001b[0m current_time \u001b[38;5;241m=\u001b[39m current_timestamp()\n\u001b[1;32m     50\u001b[0m trips_last_hour \u001b[38;5;241m=\u001b[39m parsed_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_time)\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[0;32m---> 51\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[43mexpr\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTERVAL 1 HOUR\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     52\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Convert to Pandas for spatial join\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_pandas\u001b[39m(df):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expr' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import geopandas as gpd\n",
    "\n",
    "# Define the schema for the incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", StringType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", StringType(), True),\n",
    "    StructField(\"trip_time_in_secs\", StringType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Trips Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the geojson file with GeoPandas\n",
    "boroughs_gdf = gpd.read_file('nyc-boroughs.geojson')\n",
    "\n",
    "# Broadcast the boroughs dataframe\n",
    "boroughs_broadcast = spark.sparkContext.broadcast(boroughs_gdf)\n",
    "\n",
    "# Read data from Kafka\n",
    "kafka_server = \"kafka1:9092\"\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", \"taxi_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON data and apply schema\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json\").select(from_json(\"json\", schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Filter trips within the last hour\n",
    "current_time = current_timestamp()\n",
    "trips_last_hour = parsed_df.withColumn(\"current_time\", current_time).filter(\n",
    "    (col(\"pickup_datetime\") >= (col(\"current_time\") - expr(\"INTERVAL 1 HOUR\"))) &\n",
    "    (col(\"dropoff_datetime\") <= col(\"current_time\"))\n",
    ")\n",
    "\n",
    "# Convert to Pandas for spatial join\n",
    "def to_pandas(df):\n",
    "    return df.toPandas()\n",
    "\n",
    "def spatial_join(pandas_df):\n",
    "    # Create GeoDataFrames for pickup and dropoff locations\n",
    "    pickup_gdf = gpd.GeoDataFrame(\n",
    "        pandas_df, geometry=gpd.points_from_xy(pandas_df.pickup_longitude, pandas_df.pickup_latitude))\n",
    "\n",
    "    dropoff_gdf = gpd.GeoDataFrame(\n",
    "        pandas_df, geometry=gpd.points_from_xy(pandas_df.dropoff_longitude, pandas_df.dropoff_latitude))\n",
    "\n",
    "    # Set the CRS to match the GeoJSON file\n",
    "    pickup_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    dropoff_gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    # Perform spatial join to find the borough for each pickup and dropoff location\n",
    "    pickup_with_borough = gpd.sjoin(pickup_gdf, boroughs_broadcast.value, how='left', op='within')\n",
    "    dropoff_with_borough = gpd.sjoin(dropoff_gdf, boroughs_broadcast.value, how='left', op='within')\n",
    "\n",
    "    # Add borough information to the original DataFrame\n",
    "    pandas_df['pickup_borough'] = pickup_with_borough['borough']\n",
    "    pandas_df['dropoff_borough'] = dropoff_with_borough['borough']\n",
    "\n",
    "    return pandas_df\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = to_pandas(trips_last_hour)\n",
    "\n",
    "# Perform spatial join\n",
    "joined_df = spatial_join(pandas_df)\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(joined_df)\n",
    "\n",
    "# Filter trips that start and end in the same borough\n",
    "same_borough_trips = spark_df.filter(col(\"pickup_borough\") == col(\"dropoff_borough\"))\n",
    "\n",
    "# Group by borough and count trips\n",
    "borough_trip_counts = same_borough_trips.groupBy(\"pickup_borough\").count()\n",
    "\n",
    "# Write the results to the console\n",
    "query = borough_trip_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_with_borough' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Query 3 (Corrected)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trips_within_borough_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_with_borough\u001b[49m \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwithWatermark(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_borough\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_borough\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(window(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 hour\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_borough\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mcount() \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m trips_within_borough_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_with_borough' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query 3 (Corrected)\n",
    "trips_within_borough_df = df_with_borough \\\n",
    "    .withWatermark(\"pickup_datetime\", \"10 minutes\") \\\n",
    "    .filter(col(\"pickup_borough\") == col(\"dropoff_borough\")) \\\n",
    "    .groupBy(window(\"pickup_datetime\", \"1 hour\"), \"pickup_borough\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"num_trips\")\n",
    "    \n",
    "trips_within_borough_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`partition`, `key`, `offset`, `topic`, `value`].;\n'EventTimeWatermark 'pickup_datetime, 10 minutes\n+- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@64ebf539, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@587fc8cc, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:9092, subscribe=taxi_topic], [key#69, value#70, topic#71, partition#72, offset#73L, timestamp#74, timestampType#75], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@12239223,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> taxi_topic, startingOffsets -> latest),None), kafka, [key#62, value#63, topic#64, partition#65, offset#66L, timestamp#67, timestampType#68]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trips_between_boroughs_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 minutes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_borough\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_borough\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(window(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 hour\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_borough\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_borough\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mcount() \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m trips_between_boroughs_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1147\u001b[0m, in \u001b[0;36mDataFrame.withWatermark\u001b[0;34m(self, eventTime, delayThreshold)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delayThreshold \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(delayThreshold) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1141\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1142\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         },\n\u001b[1;32m   1146\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[43meventTime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayThreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`partition`, `key`, `offset`, `topic`, `value`].;\n'EventTimeWatermark 'pickup_datetime, 10 minutes\n+- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@64ebf539, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@587fc8cc, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:9092, subscribe=taxi_topic], [key#69, value#70, topic#71, partition#72, offset#73L, timestamp#74, timestampType#75], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@12239223,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> taxi_topic, startingOffsets -> latest),None), kafka, [key#62, value#63, topic#64, partition#65, offset#66L, timestamp#67, timestampType#68]\n"
     ]
    }
   ],
   "source": [
    "trips_between_boroughs_df = df \\\n",
    "    .withWatermark(\"pickup_datetime\", \"10 minutes\") \\\n",
    "    .filter(col(\"pickup_borough\") != col(\"dropoff_borough\")) \\\n",
    "    .groupBy(window(\"pickup_datetime\", \"1 hour\"), \"pickup_borough\", \"dropoff_borough\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"num_trips\")\n",
    "    \n",
    "trips_between_boroughs_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "778e8833-f5c6-45c5-b73d-bc8ee585edac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [hack_license#155], [hack_license#155, avg(time_to_next_fare#195L) AS avg(time_to_next_fare)#217]\n+- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161, next_pickup_datetime#185, (cast(next_pickup_datetime#185 as bigint) - cast(dropoff_datetime#157 as bigint)) AS time_to_next_fare#195L]\n   +- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161, next_pickup_datetime#185]\n      +- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161, next_pickup_datetime#185, next_pickup_datetime#185]\n         +- Window [lag(pickup_datetime#156, 1, null) windowspecdefinition(hack_license#155, pickup_datetime#156 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS next_pickup_datetime#185], [hack_license#155], [pickup_datetime#156 ASC NULLS FIRST]\n            +- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161]\n               +- Project [data#152.medallion AS medallion#154, data#152.hack_license AS hack_license#155, data#152.pickup_datetime AS pickup_datetime#156, data#152.dropoff_datetime AS dropoff_datetime#157, data#152.pickup_longitude AS pickup_longitude#158, data#152.pickup_latitude AS pickup_latitude#159, data#152.dropoff_longitude AS dropoff_longitude#160, data#152.dropoff_latitude AS dropoff_latitude#161]\n                  +- Project [from_json(StructField(medallion,StringType,true), StructField(hack_license,StringType,true), StructField(pickup_datetime,TimestampType,true), StructField(dropoff_datetime,TimestampType,true), StructField(pickup_longitude,DoubleType,true), StructField(pickup_latitude,DoubleType,true), StructField(dropoff_longitude,DoubleType,true), StructField(dropoff_latitude,DoubleType,true), cast(value#139 as string), Some(Etc/UTC)) AS data#152]\n                     +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@3bc1c94c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@aae7a92, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092,kafka2:9093, subscribe=taxi_topic], [key#138, value#139, topic#140, partition#141, offset#142L, timestamp#143, timestampType#144], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@12239223,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092,kafka2:9093, subscribe -> taxi_topic, startingOffsets -> earliest),None), kafka, [key#131, value#132, topic#133, partition#134, offset#135L, timestamp#136, timestampType#137]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Start the streaming queries\u001b[39;00m\n\u001b[1;32m     62\u001b[0m utilization_query \u001b[38;5;241m=\u001b[39m utilization_df\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 63\u001b[0m avg_time_next_fare_query \u001b[38;5;241m=\u001b[39m \u001b[43mavg_time_next_fare_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m same_borough_trips_query \u001b[38;5;241m=\u001b[39m same_borough_trips_df\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     65\u001b[0m inter_borough_trips_query \u001b[38;5;241m=\u001b[39m inter_borough_trips_df\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [hack_license#155], [hack_license#155, avg(time_to_next_fare#195L) AS avg(time_to_next_fare)#217]\n+- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161, next_pickup_datetime#185, (cast(next_pickup_datetime#185 as bigint) - cast(dropoff_datetime#157 as bigint)) AS time_to_next_fare#195L]\n   +- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161, next_pickup_datetime#185]\n      +- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161, next_pickup_datetime#185, next_pickup_datetime#185]\n         +- Window [lag(pickup_datetime#156, 1, null) windowspecdefinition(hack_license#155, pickup_datetime#156 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS next_pickup_datetime#185], [hack_license#155], [pickup_datetime#156 ASC NULLS FIRST]\n            +- Project [medallion#154, hack_license#155, pickup_datetime#156, dropoff_datetime#157, pickup_longitude#158, pickup_latitude#159, dropoff_longitude#160, dropoff_latitude#161]\n               +- Project [data#152.medallion AS medallion#154, data#152.hack_license AS hack_license#155, data#152.pickup_datetime AS pickup_datetime#156, data#152.dropoff_datetime AS dropoff_datetime#157, data#152.pickup_longitude AS pickup_longitude#158, data#152.pickup_latitude AS pickup_latitude#159, data#152.dropoff_longitude AS dropoff_longitude#160, data#152.dropoff_latitude AS dropoff_latitude#161]\n                  +- Project [from_json(StructField(medallion,StringType,true), StructField(hack_license,StringType,true), StructField(pickup_datetime,TimestampType,true), StructField(dropoff_datetime,TimestampType,true), StructField(pickup_longitude,DoubleType,true), StructField(pickup_latitude,DoubleType,true), StructField(dropoff_longitude,DoubleType,true), StructField(dropoff_latitude,DoubleType,true), cast(value#139 as string), Some(Etc/UTC)) AS data#152]\n                     +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@3bc1c94c, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@aae7a92, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092,kafka2:9093, subscribe=taxi_topic], [key#138, value#139, topic#140, partition#141, offset#142L, timestamp#143, timestampType#144], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@12239223,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092,kafka2:9093, subscribe -> taxi_topic, startingOffsets -> earliest),None), kafka, [key#131, value#132, topic#133, partition#134, offset#135L, timestamp#136, timestampType#137]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Spark configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Data Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092,kafka2:9093\") \\\n",
    "    .option(\"subscribe\", \"taxi_topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON data and apply the schema\n",
    "parsed_df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Query 1: Utilization over a window of 5, 10, and 15 minutes per taxi/driver\n",
    "utilization_df = parsed_df.withWatermark(\"pickup_datetime\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        col(\"hack_license\"),\n",
    "        window(col(\"pickup_datetime\"), \"5 minutes\", \"1 minute\")\n",
    "    ).count()\n",
    "\n",
    "# Query 2: Average time to find next fare per destination borough (simplified example)\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"hack_license\").orderBy(\"pickup_datetime\")\n",
    "parsed_df = parsed_df.withColumn(\"next_pickup_datetime\", lag(\"pickup_datetime\", -1).over(window_spec))\n",
    "parsed_df = parsed_df.withColumn(\"time_to_next_fare\", col(\"next_pickup_datetime\").cast(\"long\") - col(\"dropoff_datetime\").cast(\"long\"))\n",
    "\n",
    "avg_time_next_fare_df = parsed_df.groupBy(\"hack_license\").agg({\"time_to_next_fare\": \"avg\"})\n",
    "\n",
    "# Query 3: Number of trips within the same borough in the last hour\n",
    "same_borough_trips_df = parsed_df.filter(\n",
    "    (col(\"pickup_longitude\") == col(\"dropoff_longitude\")) & (col(\"pickup_latitude\") == col(\"dropoff_latitude\"))\n",
    ").groupBy(window(col(\"pickup_datetime\"), \"1 hour\")).count()\n",
    "\n",
    "# Query 4: Number of trips from one borough to another in the last hour\n",
    "inter_borough_trips_df = parsed_df.filter(\n",
    "    (col(\"pickup_longitude\") != col(\"dropoff_longitude\")) & (col(\"pickup_latitude\") != col(\"dropoff_latitude\"))\n",
    ").groupBy(window(col(\"pickup_datetime\"), \"1 hour\")).count()\n",
    "\n",
    "# Start the streaming queries\n",
    "utilization_query = utilization_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "avg_time_next_fare_query = avg_time_next_fare_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "same_borough_trips_query = same_borough_trips_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "inter_borough_trips_query = inter_borough_trips_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Await termination\n",
    "utilization_query.awaitTermination()\n",
    "avg_time_next_fare_query.awaitTermination()\n",
    "same_borough_trips_query.awaitTermination()\n",
    "inter_borough_trips_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f754752-4ba8-460a-9b84-fb0fae437b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
