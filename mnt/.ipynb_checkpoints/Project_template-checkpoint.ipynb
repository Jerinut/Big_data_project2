{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55974225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35005b8",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "# TO MODIFY FOR YOUR SCHEMA\n",
    "schema = StructType([\n",
    "    StructField(\"license\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69712d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "# Load the DataFrame\n",
    ")\n",
    "# Parse JSON data from Kafka's 'value' column\n",
    "parsed_df = lines.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"parsed_value\")) \n",
    "\n",
    "df = parsed_df.select(\"parsed_value.*\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc",
   "metadata": {},
   "source": [
    "## The project starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65b2d7ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`license`, `total_amount`, `timestamp`].;\n'EventTimeWatermark 'pickup_datetime, 10 minutes\n+- Project [parsed_value#341.license AS license#343, parsed_value#341.total_amount AS total_amount#344, parsed_value#341.timestamp AS timestamp#345]\n   +- Project [from_json(StructField(license,StringType,true), StructField(total_amount,DoubleType,true), StructField(timestamp,TimestampType,true), value#339, Some(Etc/UTC)) AS parsed_value#341]\n      +- Project [cast(value#326 as string) AS value#339]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@48c6c486, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@477aff08, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092, subscribe=stock, maxOffsetsPerTrigger=100], [key#325, value#326, topic#327, partition#328, offset#329L, timestamp#330, timestampType#331], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@a21da24,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> stock, startingOffsets -> earliest, maxOffsetsPerTrigger -> 100),None), kafka, [key#318, value#319, topic#320, partition#321, offset#322L, timestamp#323, timestampType#324]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m window_sizes:\n\u001b[1;32m      5\u001b[0m     window_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     utilization_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 minutes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mgroupBy(window(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, window_duration), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     11\u001b[0m             (\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     12\u001b[0m             (\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m lag(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mover(Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midle_time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m         ) \\\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutilization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m coalesce(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midle_time\u001b[39m\u001b[38;5;124m\"\u001b[39m), lit(\u001b[38;5;241m0\u001b[39m))) \u001b[38;5;241m/\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Write to sink (e.g., console, file, or in-memory table for a dashboard)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     utilization_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1147\u001b[0m, in \u001b[0;36mDataFrame.withWatermark\u001b[0;34m(self, eventTime, delayThreshold)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delayThreshold \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(delayThreshold) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1141\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1142\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         },\n\u001b[1;32m   1146\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[43meventTime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayThreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pickup_datetime` cannot be resolved. Did you mean one of the following? [`license`, `total_amount`, `timestamp`].;\n'EventTimeWatermark 'pickup_datetime, 10 minutes\n+- Project [parsed_value#341.license AS license#343, parsed_value#341.total_amount AS total_amount#344, parsed_value#341.timestamp AS timestamp#345]\n   +- Project [from_json(StructField(license,StringType,true), StructField(total_amount,DoubleType,true), StructField(timestamp,TimestampType,true), value#339, Some(Etc/UTC)) AS parsed_value#341]\n      +- Project [cast(value#326 as string) AS value#339]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@48c6c486, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@477aff08, [startingOffsets=earliest, kafka.bootstrap.servers=kafka1:9092, subscribe=stock, maxOffsetsPerTrigger=100], [key#325, value#326, topic#327, partition#328, offset#329L, timestamp#330, timestampType#331], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@a21da24,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:9092, subscribe -> stock, startingOffsets -> earliest, maxOffsetsPerTrigger -> 100),None), kafka, [key#318, value#319, topic#320, partition#321, offset#322L, timestamp#323, timestampType#324]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_sizes = [5, 10, 15]  # Minutes\n",
    "for size in window_sizes:\n",
    "    window_duration = f\"{size} minutes\"\n",
    "\n",
    "    utilization_df = df \\\n",
    "        .withWatermark(\"pickup_datetime\", \"10 minutes\") \\\n",
    "        .groupBy(window(\"pickup_datetime\", window_duration), \"license\") \\\n",
    "        .agg(\n",
    "            (max(\"dropoff_datetime\") - min(\"pickup_datetime\")).cast(\"long\").alias(\"total_time\"),\n",
    "            (min(\"pickup_datetime\") - lag(\"dropoff_datetime\").over(Window.partitionBy(\"license\").orderBy(\"pickup_datetime\"))).cast(\"long\").alias(\"idle_time\")\n",
    "        ) \\\n",
    "        .withColumn(\"utilization\", (100 * (col(\"total_time\") - coalesce(col(\"idle_time\"), lit(0))) / col(\"total_time\")).cast(\"double\"))\n",
    "\n",
    "    # Write to sink (e.g., console, file, or in-memory table for a dashboard)\n",
    "    utilization_df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime='10 seconds')\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb70cb0e-316e-424a-9bfa-8dba33946b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-0.14.4-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting fiona>=1.8.21 (from geopandas)\n",
      "  Downloading fiona-1.9.6-cp311-cp311-manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m458.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.11/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from geopandas) (24.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (2.0.3)\n",
      "Collecting pyproj>=3.3.0 (from geopandas)\n",
      "  Downloading pyproj-3.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting shapely>=1.8.0 (from geopandas)\n",
      "  Downloading shapely-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (23.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (8.1.7)\n",
      "Collecting click-plugins>=1.0 (from fiona>=1.8.21->geopandas)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting cligj>=0.5 (from fiona>=1.8.21->geopandas)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Downloading geopandas-0.14.4-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fiona-1.9.6-cp311-cp311-manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyproj-3.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Installing collected packages: shapely, pyproj, cligj, click-plugins, fiona, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.6 geopandas-0.14.4 pyproj-3.6.1 shapely-2.0.4\n",
      "     boroughCode        borough  \\\n",
      "0              5  Staten Island   \n",
      "1              5  Staten Island   \n",
      "2              5  Staten Island   \n",
      "3              5  Staten Island   \n",
      "4              4         Queens   \n",
      "..           ...            ...   \n",
      "99             2          Bronx   \n",
      "100            2          Bronx   \n",
      "101            2          Bronx   \n",
      "102            2          Bronx   \n",
      "103            2          Bronx   \n",
      "\n",
      "                                                   @id  \\\n",
      "0    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "1    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "2    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "3    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "4    http://nyc.pediacities.com/Resource/Borough/Qu...   \n",
      "..                                                 ...   \n",
      "99   http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "100  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "101  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "102  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "103  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "\n",
      "                                              geometry  \n",
      "0    POLYGON ((-74.05051 40.56642, -74.04998 40.566...  \n",
      "1    POLYGON ((-74.05314 40.57770, -74.05406 40.577...  \n",
      "2    POLYGON ((-74.15946 40.64145, -74.15998 40.641...  \n",
      "3    POLYGON ((-74.08221 40.64828, -74.08142 40.648...  \n",
      "4    POLYGON ((-73.83668 40.59495, -73.83671 40.594...  \n",
      "..                                                 ...  \n",
      "99   POLYGON ((-73.78103 40.87648, -73.78121 40.876...  \n",
      "100  POLYGON ((-73.78651 40.88094, -73.78582 40.880...  \n",
      "101  POLYGON ((-73.87295 40.90444, -73.85947 40.900...  \n",
      "102  POLYGON ((-73.80518 40.81527, -73.80508 40.815...  \n",
      "103  POLYGON ((-73.80408 40.81349, -73.80424 40.813...  \n",
      "\n",
      "[104 rows x 4 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'boro_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'boro_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(gdf)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Print specific columns or rows (example)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mboro_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Print borough names\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(gdf\u001b[38;5;241m.\u001b[39mhead())       \u001b[38;5;66;03m# Print the first 5 rows\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/geopandas/geodataframe.py:1459\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1459\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# result is not geometry dtype for multi-indexes\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1463\u001b[0m         pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_scalar(key)\n\u001b[1;32m   1464\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_geometry_type(result)\n\u001b[1;32m   1468\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'boro_name'"
     ]
    }
   ],
   "source": [
    "!pip install geopandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read the GeoJSON file (replace with the actual path to your file)\n",
    "filepath = 'nyc-boroughs.geojson'\n",
    "gdf = gpd.read_file(filepath)\n",
    "\n",
    "# Print the GeoDataFrame (contains the data in a tabular format)\n",
    "print(gdf)\n",
    "\n",
    "# Print specific columns or rows (example)\n",
    "print(gdf['boro_name'])  # Print borough names\n",
    "print(gdf.head())       # Print the first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f59eda4d-6d94-41ec-8ceb-55008ec8026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     boroughCode        borough  \\\n",
      "0              5  Staten Island   \n",
      "1              5  Staten Island   \n",
      "2              5  Staten Island   \n",
      "3              5  Staten Island   \n",
      "4              4         Queens   \n",
      "..           ...            ...   \n",
      "99             2          Bronx   \n",
      "100            2          Bronx   \n",
      "101            2          Bronx   \n",
      "102            2          Bronx   \n",
      "103            2          Bronx   \n",
      "\n",
      "                                                   @id  \\\n",
      "0    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "1    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "2    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "3    http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "4    http://nyc.pediacities.com/Resource/Borough/Qu...   \n",
      "..                                                 ...   \n",
      "99   http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "100  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "101  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "102  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "103  http://nyc.pediacities.com/Resource/Borough/Bronx   \n",
      "\n",
      "                                              geometry  \n",
      "0    POLYGON ((-74.05051 40.56642, -74.04998 40.566...  \n",
      "1    POLYGON ((-74.05314 40.57770, -74.05406 40.577...  \n",
      "2    POLYGON ((-74.15946 40.64145, -74.15998 40.641...  \n",
      "3    POLYGON ((-74.08221 40.64828, -74.08142 40.648...  \n",
      "4    POLYGON ((-73.83668 40.59495, -73.83671 40.594...  \n",
      "..                                                 ...  \n",
      "99   POLYGON ((-73.78103 40.87648, -73.78121 40.876...  \n",
      "100  POLYGON ((-73.78651 40.88094, -73.78582 40.880...  \n",
      "101  POLYGON ((-73.87295 40.90444, -73.85947 40.900...  \n",
      "102  POLYGON ((-73.80518 40.81527, -73.80508 40.815...  \n",
      "103  POLYGON ((-73.80408 40.81349, -73.80424 40.813...  \n",
      "\n",
      "[104 rows x 4 columns]\n",
      "0      Staten Island\n",
      "1      Staten Island\n",
      "2      Staten Island\n",
      "3      Staten Island\n",
      "4             Queens\n",
      "           ...      \n",
      "99             Bronx\n",
      "100            Bronx\n",
      "101            Bronx\n",
      "102            Bronx\n",
      "103            Bronx\n",
      "Name: borough, Length: 104, dtype: object\n",
      "   boroughCode        borough  \\\n",
      "0            5  Staten Island   \n",
      "1            5  Staten Island   \n",
      "2            5  Staten Island   \n",
      "3            5  Staten Island   \n",
      "4            4         Queens   \n",
      "\n",
      "                                                 @id  \\\n",
      "0  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "1  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "2  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "3  http://nyc.pediacities.com/Resource/Borough/St...   \n",
      "4  http://nyc.pediacities.com/Resource/Borough/Qu...   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((-74.05051 40.56642, -74.04998 40.566...  \n",
      "1  POLYGON ((-74.05314 40.57770, -74.05406 40.577...  \n",
      "2  POLYGON ((-74.15946 40.64145, -74.15998 40.641...  \n",
      "3  POLYGON ((-74.08221 40.64828, -74.08142 40.648...  \n",
      "4  POLYGON ((-73.83668 40.59495, -73.83671 40.594...  \n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Read the GeoJSON file (replace with the actual path to your file)\n",
    "filepath = 'nyc-boroughs.geojson'\n",
    "gdf = gpd.read_file(filepath)\n",
    "\n",
    "# Find the actual column name for borough\n",
    "boro_name_col = 'borough' # Replace with the correct name found in the GeoJSON file\n",
    "\n",
    "# Print the GeoDataFrame (contains the data in a tabular format)\n",
    "print(gdf)\n",
    "\n",
    "# Print specific columns or rows (example)\n",
    "print(gdf[boro_name_col])  # Use the correct column name\n",
    "print(gdf.head())       # Print the first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Load borough boundaries (replace with the correct path to your GeoJSON file)\n",
    "boroughs_gdf = gpd.read_file('nyc-boroughs.geojson')\n",
    "\n",
    "# Function to determine borough from coordinates (same as before)\n",
    "def get_borough(longitude, latitude):\n",
    "    point = Point(longitude, latitude)\n",
    "    for idx, borough in boroughs_gdf.iterrows():\n",
    "        if borough['geometry'].contains(point):\n",
    "            return borough['boro_name']\n",
    "    return 'Unknown'\n",
    "\n",
    "# UDF to add dropoff_borough column\n",
    "get_borough_udf = udf(get_borough, StringType())\n",
    "df_with_borough = df.withColumn(\"dropoff_borough\", get_borough_udf(\"dropoff_longitude\", \"dropoff_latitude\"))\n",
    "\n",
    "# Calculate average time to next fare (using df_with_borough instead of df)\n",
    "time_to_next_fare_df = df_with_borough \\\n",
    "    .withWatermark(\"pickup_datetime\", \"10 minutes\") \\\n",
    "    .select(\"license\", \"dropoff_datetime\", \"dropoff_borough\") \\\n",
    "    .join(\n",
    "        df_with_borough.selectExpr(\"license\", \"pickup_datetime AS next_pickup_datetime\"), \n",
    "        on=[\"license\"], how=\"inner\"\n",
    "    ) \\\n",
    "    .filter(col(\"dropoff_datetime\") < col(\"next_pickup_datetime\")) \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"1 hour\"), \"dropoff_borough\") \\\n",
    "    .agg(\n",
    "        (mean(col(\"next_pickup_datetime\").cast(\"long\") - col(\"dropoff_datetime\").cast(\"long\")) / 1000).alias(\"avg_time_to_next_fare\")\n",
    "    )\n",
    "\n",
    "# Output results (same as before)\n",
    "time_to_next_fare_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12b62c69-d802-4742-8ba0-10db8a35367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.11/site-packages (0.14.4)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.11/site-packages (2.0.4)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /opt/conda/lib/python3.11/site-packages (from geopandas) (1.9.6)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.11/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from geopandas) (24.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (2.0.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/lib/python3.11/site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (23.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3517: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3517: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          medallion                      hack_license  \\\n",
      "0  89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
      "1  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
      "2  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
      "3  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
      "4  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
      "\n",
      "  vendor_id  rate_code store_and_fwd_flag      pickup_datetime  \\\n",
      "0       CMT          1                  N  2013-01-01 15:11:48   \n",
      "1       CMT          1                  N  2013-01-06 00:18:35   \n",
      "2       CMT          1                  N  2013-01-05 18:49:41   \n",
      "3       CMT          1                  N  2013-01-07 23:54:15   \n",
      "4       CMT          1                  N  2013-01-07 23:25:03   \n",
      "\n",
      "      dropoff_datetime  passenger_count  trip_time_in_secs  trip_distance  \\\n",
      "0  2013-01-01 15:18:10                4                382            1.0   \n",
      "1  2013-01-06 00:22:54                1                259            1.5   \n",
      "2  2013-01-05 18:54:23                1                282            1.1   \n",
      "3  2013-01-07 23:58:20                2                244            0.7   \n",
      "4  2013-01-07 23:34:24                1                560            2.1   \n",
      "\n",
      "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
      "0        -73.978165        40.757977         -73.989838         40.751171   \n",
      "1        -74.006683        40.731781         -73.994499         40.750660   \n",
      "2        -74.004707        40.737770         -74.009834         40.726002   \n",
      "3        -73.974602        40.759945         -73.984734         40.759388   \n",
      "4        -73.976250        40.748528         -74.002586         40.747868   \n",
      "\n",
      "  pickup_borough dropoff_borough  \n",
      "0      Manhattan       Manhattan  \n",
      "1      Manhattan       Manhattan  \n",
      "2      Manhattan       Manhattan  \n",
      "3      Manhattan       Manhattan  \n",
      "4      Manhattan       Manhattan  \n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas shapely\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the GeoJSON file\n",
    "boroughs_gdf = gpd.read_file('nyc-boroughs.geojson')\n",
    "\n",
    "# Load the taxi data CSV file\n",
    "taxi_df = pd.read_csv('sample.csv')\n",
    "\n",
    "# Create GeoDataFrames for pickup and dropoff locations\n",
    "pickup_gdf = gpd.GeoDataFrame(\n",
    "    taxi_df, geometry=gpd.points_from_xy(taxi_df.pickup_longitude, taxi_df.pickup_latitude))\n",
    "\n",
    "dropoff_gdf = gpd.GeoDataFrame(\n",
    "    taxi_df, geometry=gpd.points_from_xy(taxi_df.dropoff_longitude, taxi_df.dropoff_latitude))\n",
    "\n",
    "# Set the coordinate reference system (CRS) to match the GeoJSON file\n",
    "pickup_gdf.set_crs(epsg=4326, inplace=True)\n",
    "dropoff_gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Perform spatial join to find the borough for each pickup and dropoff location\n",
    "pickup_with_borough = gpd.sjoin(pickup_gdf, boroughs_gdf, how='left', op='within')\n",
    "dropoff_with_borough = gpd.sjoin(dropoff_gdf, boroughs_gdf, how='left', op='within')\n",
    "\n",
    "# Extract the borough information\n",
    "taxi_df['pickup_borough'] = pickup_with_borough['borough']\n",
    "taxi_df['dropoff_borough'] = dropoff_with_borough['borough']\n",
    "\n",
    "# Display the DataFrame with borough information\n",
    "print(taxi_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f79ad-6b2c-4969-b8f1-3f5eb25317a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import geopandas as gpd\n",
    "\n",
    "# Define the schema for the incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"rate_code\", StringType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", StringType(), True),\n",
    "    StructField(\"trip_time_in_secs\", StringType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Trips Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the geojson file with GeoPandas\n",
    "boroughs_gdf = gpd.read_file('nyc-boroughs.geojson')\n",
    "\n",
    "# Broadcast the boroughs dataframe\n",
    "boroughs_broadcast = spark.sparkContext.broadcast(boroughs_gdf)\n",
    "\n",
    "# Read data from Kafka\n",
    "kafka_server = \"kafka1:9092\"\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", \"taxi_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON data and apply schema\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json\").select(from_json(\"json\", schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Filter trips within the last hour\n",
    "current_time = current_timestamp()\n",
    "trips_last_hour = parsed_df.withColumn(\"current_time\", current_time).filter(\n",
    "    (col(\"pickup_datetime\") >= (col(\"current_time\") - expr(\"INTERVAL 1 HOUR\"))) &\n",
    "    (col(\"dropoff_datetime\") <= col(\"current_time\"))\n",
    ")\n",
    "\n",
    "# Convert to Pandas for spatial join\n",
    "def to_pandas(df):\n",
    "    return df.toPandas()\n",
    "\n",
    "def spatial_join(pandas_df):\n",
    "    # Create GeoDataFrames for pickup and dropoff locations\n",
    "    pickup_gdf = gpd.GeoDataFrame(\n",
    "        pandas_df, geometry=gpd.points_from_xy(pandas_df.pickup_longitude, pandas_df.pickup_latitude))\n",
    "\n",
    "    dropoff_gdf = gpd.GeoDataFrame(\n",
    "        pandas_df, geometry=gpd.points_from_xy(pandas_df.dropoff_longitude, pandas_df.dropoff_latitude))\n",
    "\n",
    "    # Set the CRS to match the GeoJSON file\n",
    "    pickup_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    dropoff_gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    # Perform spatial join to find the borough for each pickup and dropoff location\n",
    "    pickup_with_borough = gpd.sjoin(pickup_gdf, boroughs_broadcast.value, how='left', op='within')\n",
    "    dropoff_with_borough = gpd.sjoin(dropoff_gdf, boroughs_broadcast.value, how='left', op='within')\n",
    "\n",
    "    # Add borough information to the original DataFrame\n",
    "    pandas_df['pickup_borough'] = pickup_with_borough['borough']\n",
    "    pandas_df['dropoff_borough'] = dropoff_with_borough['borough']\n",
    "\n",
    "    return pandas_df\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = to_pandas(trips_last_hour)\n",
    "\n",
    "# Perform spatial join\n",
    "joined_df = spatial_join(pandas_df)\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(joined_df)\n",
    "\n",
    "# Filter trips that start and end in the same borough\n",
    "same_borough_trips = spark_df.filter(col(\"pickup_borough\") == col(\"dropoff_borough\"))\n",
    "\n",
    "# Group by borough and count trips\n",
    "borough_trip_counts = same_borough_trips.groupBy(\"pickup_borough\").count()\n",
    "\n",
    "# Write the results to the console\n",
    "query = borough_trip_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query 3 (Corrected)\n",
    "trips_within_borough_df = df_with_borough \\\n",
    "    .withWatermark(\"pickup_datetime\", \"10 minutes\") \\\n",
    "    .filter(col(\"pickup_borough\") == col(\"dropoff_borough\")) \\\n",
    "    .groupBy(window(\"pickup_datetime\", \"1 hour\"), \"pickup_borough\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"num_trips\")\n",
    "    \n",
    "trips_within_borough_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_between_boroughs_df = df \\\n",
    "    .withWatermark(\"pickup_datetime\", \"10 minutes\") \\\n",
    "    .filter(col(\"pickup_borough\") != col(\"dropoff_borough\")) \\\n",
    "    .groupBy(window(\"pickup_datetime\", \"1 hour\"), \"pickup_borough\", \"dropoff_borough\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"num_trips\")\n",
    "    \n",
    "trips_between_boroughs_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e8833-f5c6-45c5-b73d-bc8ee585edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Spark configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Data Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the incoming data\n",
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092,kafka2:9093\") \\\n",
    "    .option(\"subscribe\", \"taxi_topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON data and apply the schema\n",
    "parsed_df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Query 1: Utilization over a window of 5, 10, and 15 minutes per taxi/driver\n",
    "utilization_df = parsed_df.withWatermark(\"pickup_datetime\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        col(\"hack_license\"),\n",
    "        window(col(\"pickup_datetime\"), \"5 minutes\", \"1 minute\")\n",
    "    ).count()\n",
    "\n",
    "# Query 2: Average time to find next fare per destination borough (simplified example)\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"hack_license\").orderBy(\"pickup_datetime\")\n",
    "parsed_df = parsed_df.withColumn(\"next_pickup_datetime\", lag(\"pickup_datetime\", -1).over(window_spec))\n",
    "parsed_df = parsed_df.withColumn(\"time_to_next_fare\", col(\"next_pickup_datetime\").cast(\"long\") - col(\"dropoff_datetime\").cast(\"long\"))\n",
    "\n",
    "avg_time_next_fare_df = parsed_df.groupBy(\"hack_license\").agg({\"time_to_next_fare\": \"avg\"})\n",
    "\n",
    "# Query 3: Number of trips within the same borough in the last hour\n",
    "same_borough_trips_df = parsed_df.filter(\n",
    "    (col(\"pickup_longitude\") == col(\"dropoff_longitude\")) & (col(\"pickup_latitude\") == col(\"dropoff_latitude\"))\n",
    ").groupBy(window(col(\"pickup_datetime\"), \"1 hour\")).count()\n",
    "\n",
    "# Query 4: Number of trips from one borough to another in the last hour\n",
    "inter_borough_trips_df = parsed_df.filter(\n",
    "    (col(\"pickup_longitude\") != col(\"dropoff_longitude\")) & (col(\"pickup_latitude\") != col(\"dropoff_latitude\"))\n",
    ").groupBy(window(col(\"pickup_datetime\"), \"1 hour\")).count()\n",
    "\n",
    "# Start the streaming queries\n",
    "utilization_query = utilization_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "avg_time_next_fare_query = avg_time_next_fare_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "same_borough_trips_query = same_borough_trips_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "inter_borough_trips_query = inter_borough_trips_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# Await termination\n",
    "utilization_query.awaitTermination()\n",
    "avg_time_next_fare_query.awaitTermination()\n",
    "same_borough_trips_query.awaitTermination()\n",
    "inter_borough_trips_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89274b0-015c-4aea-a033-1fb5b6464a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
