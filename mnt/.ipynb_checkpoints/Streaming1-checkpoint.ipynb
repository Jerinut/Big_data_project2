{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"DF2_Practice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True) # OK for exploration, not great for performance\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b131d293-c37a-4753-95e3-85a755db6aaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Delta Lake continued, Intro to Structured Streaming\n",
    "\n",
    "* Delta Lake\n",
    "    * Data Lake to Data Warehouse: merge into\n",
    "    * Updating table definition\n",
    "    * Partition column\n",
    "* Structured Streaming intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29997c12-5edd-4bf4-9d29-e66e6d1c96ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We will use a movies dataset and try to simulate dimensional modelling behaviour\n",
    "\n",
    "# we don't need to create a separate variable for our dataframes\n",
    "# if we have done testing/debugging, we can be more concise\n",
    "\n",
    "(spark.read\n",
    " .option(\"delimiter\",\"::\")\n",
    " .option(\"inferSchema\",\"true\")\n",
    " .csv(\"movies/movies.dat\")\n",
    " .toDF(\"MovieID\",\"Title\",\"Genres\") # for naming columns\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_movies\")\n",
    ")\n",
    "\n",
    "(spark.read\n",
    " .option(\"delimiter\", \"::\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .csv(\"movies/ratings.dat\")\n",
    " .toDF(\"UserID\",\"MovieID\",\"Rating\",\"Timestamp\")\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings\")\n",
    ")\n",
    "\n",
    "(spark.read\n",
    " .option(\"delimiter\", \"::\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .csv(\"movies/users.dat\")\n",
    " .toDF(\"UserID\",\"Gender\",\"Age\",\"Occupation\",\"Zip-code\")\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_users\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377629c8-db8b-4027-b7e9-8910cfcf6d37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's separate data into smaller batches for simulating incremental inserts\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2000-01-01','2000-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2000\")\n",
    ")\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2001-01-01','2001-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2001\")\n",
    ")\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2002-01-01','2002-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2002\")\n",
    ")\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .filter(F.col(\"timestamp\").cast(\"timestamp\").cast(\"date\").between('2003-01-01','2003-12-31'))\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(\"source_ratings_2003\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea64d44a-dad2-444f-83ad-65e069472902",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can create dimension and fact tables. We will only use year 2000 data for now\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE DimUser USING DELTA AS\n",
    "SELECT *\n",
    ", CAST('2001-01-01' as date) ValidFrom\n",
    ", CAST('9999-12-31' as date) ValidTo\n",
    "FROM source_users\n",
    "WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE DimMovie USING DELTA  AS\n",
    "SELECT *\n",
    ", CAST('2001-01-01' as date) ValidFrom\n",
    ", CAST('9999-12-31' as date) ValidTo\n",
    "FROM source_movies\n",
    "WHERE movieID IN (SELECT movieID FROM source_ratings_2000)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE FactRating USING DELTA  AS\n",
    "SELECT *\n",
    "FROM source_ratings_2000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f4d2995-613d-40bb-b626-3c18bf79a93b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>num_affected_rows</th></tr>\n",
       "<tr><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+\n",
       "|num_affected_rows|\n",
       "+-----------------+\n",
       "|                1|\n",
       "+-----------------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make some modifications to the data to see how handling slowly changing dimensions would work\n",
    "\n",
    "spark.sql(\"\"\"UPDATE source_users\n",
    "SET Age = 21\n",
    "WHERE Age = 1\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"UPDATE source_users\n",
    "SET `Zip-code` = 12345\n",
    "WHERE `Zip-Code` = 10023\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"UPDATE source_movies\n",
    "SET Genres = 'Comedy'\n",
    "WHERE MovieID = 18\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9c63108-a666-4927-9727-e87935713bce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr>\n",
       "<tr><td>230</td><td>0</td><td>0</td><td>230</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+----------------+----------------+-----------------+\n",
       "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
       "+-----------------+----------------+----------------+-----------------+\n",
       "|              230|               0|               0|              230|\n",
       "+-----------------+----------------+----------------+-----------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the MERGE statement is a powerful way of upserting data:\n",
    "# in the first statement, if the userid exists but something about the user has changed, then we invalidate the old data\n",
    "# and if the userid does not exist, then we insert the new user\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO DimUser tgt\n",
    "USING (\n",
    "  SELECT * FROM source_users\n",
    "  WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n",
    "  OR UserID IN (SELECT UserID FROM source_ratings_2001)\n",
    ") src\n",
    "ON tgt.UserID = src.UserID\n",
    "WHEN MATCHED AND (tgt.Gender != src.Gender OR tgt.Age != src.Age OR tgt.Occupation != src.Occupation OR tgt.`Zip-code` != src.`Zip-code`)\n",
    "  THEN UPDATE SET tgt.ValidTo = '2002-01-01'\n",
    "WHEN NOT MATCHED THEN INSERT (UserID, Gender, Age, Occupation, `Zip-code`, ValidFrom, ValidTo) VALUES ( \n",
    "          src.UserID\n",
    "          , src.Gender\n",
    "          , src.Age\n",
    "          , src.Occupation\n",
    "          , src.`Zip-code`\n",
    "          , CAST('2002-01-01' as date) \n",
    "          , CAST('9999-12-31' as date)\n",
    "          )\n",
    "\"\"\")\n",
    "\n",
    "# in this statement, we add new data for all the rows that we invalidated with the previous statement\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO DimUser tgt\n",
    "USING (\n",
    "  SELECT * FROM source_users\n",
    "  WHERE UserID IN (SELECT UserID FROM source_ratings_2000)\n",
    "  OR UserID IN (SELECT UserID FROM source_ratings_2001)\n",
    ") src\n",
    "ON tgt.UserID = src.UserID\n",
    "AND tgt.Gender = src.Gender\n",
    "AND tgt.Age = src.Age\n",
    "AND tgt.Occupation = src.Occupation\n",
    "AND tgt.`Zip-code` = src.`Zip-code`\n",
    "WHEN NOT MATCHED THEN INSERT (UserID, Gender, Age, Occupation, `Zip-code`, ValidFrom, ValidTo) VALUES ( \n",
    "          src.UserID\n",
    "          , src.Gender\n",
    "          , src.Age\n",
    "          , src.Occupation\n",
    "          , src.`Zip-code`\n",
    "          , CAST('2002-01-01' as date) \n",
    "          , CAST('9999-12-31' as date)\n",
    "          )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ea1878-cddb-458d-81d5-a62118127c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using the Delta API\n",
    "# the queries follow the same pattern as in the previous cell\n",
    "\n",
    "dimMovieTable = DeltaTable.forName(spark, \"dimMovie\")\n",
    "sourceMoviesDf = spark.sql(\"\"\"\n",
    "SELECT * FROM source_movies\n",
    "WHERE MovieID IN (SELECT MovieID FROM source_ratings_2000)\n",
    "OR MovieID IN (SELECT MovieID FROM source_ratings_2001)\n",
    "\"\"\")\n",
    "\n",
    "dimMovieTable.alias(\"tgt\").merge(\n",
    "  source = sourceMoviesDf.alias(\"src\"),\n",
    "  condition = \"tgt.MovieID = src.MovieID\"\n",
    ").whenMatchedUpdate(\n",
    "  condition = \"tgt.Title != src.Title OR tgt.Genres != src.Genres\",\n",
    "  set = \n",
    "  {\n",
    "   \"ValidTo\": \"cast('2002-01-01' as date)\"\n",
    "  }\n",
    ").whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"MovieID\": \"src.MovieID\",\n",
    "      \"Title\": \"src.Title\",\n",
    "      \"Genres\": \"src.Genres\",\n",
    "      \"ValidFrom\": \"cast('2002-01-01' as date)\",\n",
    "      \"ValidTo\": \"cast('9999-12-31' as date)\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "\n",
    "dimMovieTable.alias(\"tgt\").merge(\n",
    "  source = sourceMoviesDf.alias(\"src\"),\n",
    "  condition = \"tgt.MovieID = src.MovieID AND tgt.Title = src.Title AND tgt.Genres = src.Genres\"\n",
    ").whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"MovieID\": \"src.MovieID\",\n",
    "      \"Title\": \"src.Title\",\n",
    "      \"Genres\": \"src.Genres\",\n",
    "      \"ValidFrom\": \"cast('2002-01-01' as date)\",\n",
    "      \"ValidTo\": \"cast('9999-12-31' as date)\"\n",
    "    }\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- MovieID: integer (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema evolution - this cell is for reference on how schema is currently \n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c110656-c67a-4fbc-8762-08cb8b911da1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- MovieID: integer (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema evolution\n",
    "# our source data now includes a new column - date\n",
    "# we can use mergeSchema to \n",
    "\n",
    "df = (spark.table(\"source_ratings_2001\")\n",
    "      .select(\"*\", \n",
    "              F.col(\"timestamp\")\n",
    "              .cast(\"timestamp\")\n",
    "              .cast(\"date\")\n",
    "              .alias(\"date\")\n",
    "             )\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"FactRating\")\n",
    "     )\n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>UserID</th><th>MovieID</th><th>Rating</th><th>Timestamp</th><th>date</th></tr>\n",
       "<tr><td>2035</td><td>2202</td><td>4</td><td>979245334</td><td>2001-01-11</td></tr>\n",
       "<tr><td>2035</td><td>2291</td><td>2</td><td>980114982</td><td>2001-01-21</td></tr>\n",
       "<tr><td>2035</td><td>2903</td><td>2</td><td>979245309</td><td>2001-01-11</td></tr>\n",
       "<tr><td>2036</td><td>912</td><td>5</td><td>993957674</td><td>2001-07-01</td></tr>\n",
       "<tr><td>2036</td><td>1449</td><td>5</td><td>993957620</td><td>2001-07-01</td></tr>\n",
       "<tr><td>2036</td><td>3083</td><td>5</td><td>993957620</td><td>2001-07-01</td></tr>\n",
       "<tr><td>2036</td><td>1197</td><td>5</td><td>993957743</td><td>2001-07-01</td></tr>\n",
       "<tr><td>2036</td><td>3129</td><td>3</td><td>993957620</td><td>2001-07-01</td></tr>\n",
       "<tr><td>2036</td><td>1247</td><td>5</td><td>993957700</td><td>2001-07-01</td></tr>\n",
       "<tr><td>2041</td><td>2987</td><td>4</td><td>1004131358</td><td>2001-10-26</td></tr>\n",
       "<tr><td>2041</td><td>1253</td><td>5</td><td>983032381</td><td>2001-02-24</td></tr>\n",
       "<tr><td>2041</td><td>1257</td><td>3</td><td>978575993</td><td>2001-01-04</td></tr>\n",
       "<tr><td>2041</td><td>2997</td><td>4</td><td>978575690</td><td>2001-01-04</td></tr>\n",
       "<tr><td>2041</td><td>587</td><td>3</td><td>979180575</td><td>2001-01-11</td></tr>\n",
       "<tr><td>2041</td><td>588</td><td>4</td><td>978576063</td><td>2001-01-04</td></tr>\n",
       "<tr><td>2041</td><td>3000</td><td>4</td><td>1002928294</td><td>2001-10-12</td></tr>\n",
       "<tr><td>2041</td><td>1</td><td>4</td><td>978575760</td><td>2001-01-04</td></tr>\n",
       "<tr><td>2041</td><td>2202</td><td>5</td><td>983032248</td><td>2001-02-24</td></tr>\n",
       "<tr><td>2041</td><td>6</td><td>4</td><td>1002928109</td><td>2001-10-12</td></tr>\n",
       "<tr><td>2041</td><td>3006</td><td>4</td><td>979179921</td><td>2001-01-11</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------+-------+------+----------+----------+\n",
       "|UserID|MovieID|Rating| Timestamp|      date|\n",
       "+------+-------+------+----------+----------+\n",
       "|  2035|   2202|     4| 979245334|2001-01-11|\n",
       "|  2035|   2291|     2| 980114982|2001-01-21|\n",
       "|  2035|   2903|     2| 979245309|2001-01-11|\n",
       "|  2036|    912|     5| 993957674|2001-07-01|\n",
       "|  2036|   1449|     5| 993957620|2001-07-01|\n",
       "|  2036|   3083|     5| 993957620|2001-07-01|\n",
       "|  2036|   1197|     5| 993957743|2001-07-01|\n",
       "|  2036|   3129|     3| 993957620|2001-07-01|\n",
       "|  2036|   1247|     5| 993957700|2001-07-01|\n",
       "|  2041|   2987|     4|1004131358|2001-10-26|\n",
       "|  2041|   1253|     5| 983032381|2001-02-24|\n",
       "|  2041|   1257|     3| 978575993|2001-01-04|\n",
       "|  2041|   2997|     4| 978575690|2001-01-04|\n",
       "|  2041|    587|     3| 979180575|2001-01-11|\n",
       "|  2041|    588|     4| 978576063|2001-01-04|\n",
       "|  2041|   3000|     4|1002928294|2001-10-12|\n",
       "|  2041|      1|     4| 978575760|2001-01-04|\n",
       "|  2041|   2202|     5| 983032248|2001-02-24|\n",
       "|  2041|      6|     4|1002928109|2001-10-12|\n",
       "|  2041|   3006|     4| 979179921|2001-01-11|\n",
       "+------+-------+------+----------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.table(\"FactRating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0cf9819-2e58-4cd6-a950-efd19b0d9e8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- MovieID: integer (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- user: integer (nullable = true)\n",
      " |-- utc_timestamp: timestamp (nullable = true)\n",
      " |-- utc_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema overwriting - mergeSchema is probably not what you want\n",
    "# (here, the column names have changed; however, with mergeSchema, the old columns remain)\n",
    "\n",
    "df = (spark.table(\"source_ratings\")\n",
    "     .select(F.col(\"userID\").alias(\"user\"), \n",
    "              F.col(\"timestamp\")\n",
    "              .cast(\"timestamp\")\n",
    "              .alias(\"utc_timestamp\"), \n",
    "              F.col(\"timestamp\")\n",
    "              .cast(\"timestamp\")\n",
    "              .cast(\"date\")\n",
    "              .alias(\"utc_date\")\n",
    "             )\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"FactRating\")\n",
    "     )\n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>UserID</th><th>MovieID</th><th>Rating</th><th>Timestamp</th><th>date</th><th>user</th><th>utc_timestamp</th><th>utc_date</th></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 20:03:11</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 20:04:44</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 16:15:00</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:14:49</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:58:36</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:31:38</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 16:21:50</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-13 00:19:09</td><td>2000-09-13</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 16:13:05</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:34:44</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-13 00:13:37</td><td>2000-09-13</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:36:42</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:48:56</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:50:01</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-13 00:19:09</td><td>2000-09-13</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 16:14:32</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:50:32</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-13 00:00:40</td><td>2000-09-13</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-12 23:43:27</td><td>2000-09-12</td></tr>\n",
       "<tr><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>3180</td><td>2000-09-13 00:08:30</td><td>2000-09-13</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------+-------+------+---------+----+----+-------------------+----------+\n",
       "|UserID|MovieID|Rating|Timestamp|date|user|      utc_timestamp|  utc_date|\n",
       "+------+-------+------+---------+----+----+-------------------+----------+\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 20:03:11|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 20:04:44|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 16:15:00|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:14:49|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:58:36|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:31:38|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 16:21:50|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-13 00:19:09|2000-09-13|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 16:13:05|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:34:44|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-13 00:13:37|2000-09-13|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:36:42|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:48:56|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:50:01|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-13 00:19:09|2000-09-13|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 16:14:32|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:50:32|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-13 00:00:40|2000-09-13|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-12 23:43:27|2000-09-12|\n",
       "|  NULL|   NULL|  NULL|     NULL|NULL|3180|2000-09-13 00:08:30|2000-09-13|\n",
       "+------+-------+------+---------+----+----+-------------------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# because we used mode(\"overwrite\"), all of the data was overwritten\n",
    "# but the old schema remained as a rudiment. Thus, all of the old columns are now NULL\n",
    "display(spark.table(\"FactRating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49e23263-c4eb-4fcf-be33-1dc3c384c36b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: integer (nullable = true)\n",
      " |-- utc_timestamp: timestamp (nullable = true)\n",
      " |-- utc_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instead, if you need to completely overwrite the schema, use overwriteSchema\n",
    "# note: append mode is not allowed\n",
    "\n",
    "(spark.table(\"source_ratings\")\n",
    " .select(F.col(\"userID\").alias(\"user\"), \n",
    "          F.col(\"timestamp\")\n",
    "          .cast(\"timestamp\")\n",
    "          .alias(\"utc_timestamp\"), \n",
    "          F.col(\"timestamp\")\n",
    "          .cast(\"timestamp\")\n",
    "          .cast(\"date\")\n",
    "          .alias(\"utc_date\")\n",
    "         )\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"overwriteSchema\", \"true\")\n",
    "#  .partitionBy(\"utc_date\") # column partition - most commonly on date, needs to be not too unique. Ideally something you use in filters a lot. Only makes sense if it would be ~ 1 GB per partition!\n",
    "  .saveAsTable(\"FactRating\")\n",
    " )\n",
    "\n",
    "spark.table(\"FactRating\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72792651-d7e3-45df-9a60-d7086844e07b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user</th><th>utc_timestamp</th><th>utc_date</th></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:06:39</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:05:22</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:53:52</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:06:39</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:55:12</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:05:48</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:49:22</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:02:06</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:05:48</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:51:46</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:54:20</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:05:22</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:57:43</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:53:30</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:05:22</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:02:28</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 17:56:13</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:07:36</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:07:08</td><td>2000-11-22</td></tr>\n",
       "<tr><td>1114</td><td>2000-11-22 18:01:13</td><td>2000-11-22</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----+-------------------+----------+\n",
       "|user|      utc_timestamp|  utc_date|\n",
       "+----+-------------------+----------+\n",
       "|1114|2000-11-22 18:06:39|2000-11-22|\n",
       "|1114|2000-11-22 18:05:22|2000-11-22|\n",
       "|1114|2000-11-22 17:53:52|2000-11-22|\n",
       "|1114|2000-11-22 18:06:39|2000-11-22|\n",
       "|1114|2000-11-22 17:55:12|2000-11-22|\n",
       "|1114|2000-11-22 18:05:48|2000-11-22|\n",
       "|1114|2000-11-22 17:49:22|2000-11-22|\n",
       "|1114|2000-11-22 18:02:06|2000-11-22|\n",
       "|1114|2000-11-22 18:05:48|2000-11-22|\n",
       "|1114|2000-11-22 17:51:46|2000-11-22|\n",
       "|1114|2000-11-22 17:54:20|2000-11-22|\n",
       "|1114|2000-11-22 18:05:22|2000-11-22|\n",
       "|1114|2000-11-22 17:57:43|2000-11-22|\n",
       "|1114|2000-11-22 17:53:30|2000-11-22|\n",
       "|1114|2000-11-22 18:05:22|2000-11-22|\n",
       "|1114|2000-11-22 18:02:28|2000-11-22|\n",
       "|1114|2000-11-22 17:56:13|2000-11-22|\n",
       "|1114|2000-11-22 18:07:36|2000-11-22|\n",
       "|1114|2000-11-22 18:07:08|2000-11-22|\n",
       "|1114|2000-11-22 18:01:13|2000-11-22|\n",
       "+----+-------------------+----------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.table(\"FactRating\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about Delta Lake optimizations:  \n",
    "https://docs.delta.io/latest/optimizations-oss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d095919-1574-47e6-bd89-58298304e99f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Structured streaming\n",
    "\n",
    "Common input/output:\n",
    "  * Kafka (and other distributed commit logs)\n",
    "  * Files (Parquet, ORC, Avro, JSON, ...)\n",
    "  * (Delta) tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a000241-a9e5-4dbd-9e47-e3b4db029f33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: events_parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's start by looking at reading and writing streams to files\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# we need a schema when reading streams\u001b[39;00m\n\u001b[1;32m      4\u001b[0m events_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m streaming_events_df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxFilesPerTrigger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# used for example purposes, reads in 1 file per trigger\u001b[39;49;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevents_parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:523\u001b[0m, in \u001b[0;36mDataStreamReader.parquet\u001b[0;34m(self, path, mergeSchema, pathGlobFilter, recursiveFileLookup, datetimeRebaseMode, int96RebaseMode)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    516\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    517\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    526\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    527\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    528\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: events_parquet."
     ]
    }
   ],
   "source": [
    "# let's start by looking at reading and writing streams to files\n",
    "# we need a schema when reading streams\n",
    "\n",
    "events_schema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n",
    "\n",
    "streaming_events_df = (spark.readStream\n",
    "  .schema(events_schema)\n",
    "  .option(\"maxFilesPerTrigger\", 1) # used for example purposes, reads in 1 file per trigger\n",
    "  .parquet(\"events_parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd55451d-fefc-4712-bffd-a92391082893",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# you can check if a dataframe has streaming sources\n",
    "# this means some functions are unavailable (eg count)\n",
    "\n",
    "streaming_events_df.isStreaming\n",
    "\n",
    "\n",
    "# displaying data\n",
    "# in Databricks, we can use display(df) for debugging\n",
    "# running locally, you can debug output to console (note, does not work when using Jupyter)\n",
    "#streaming_events_df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "609a521d-6546-4d7b-80c3-1ead4eff0aea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's create a new dataframe and write it into a file\n",
    "\n",
    "email_df = (streaming_events_df\n",
    "            .filter(F.col(\"traffic_source\") == \"email\")\n",
    "            .withColumn(\"mobile\", F.col(\"device\").isin([\"iOS\", \"Android\"]))\n",
    "            .select(\"user_id\", \"event_timestamp\", \"mobile\")\n",
    "           )\n",
    "\n",
    "checkpoint_path = \"streaming/email_traffic/checkpoint\" \n",
    "output_path = \"streaming/email_traffic/output\"\n",
    "\n",
    "devices_query = (email_df.writeStream\n",
    "  .outputMode(\"append\") # append = only new rows, complete = all rows written on every update, update = only updated rows (used in aggregations, otherwise same as append)\n",
    "  .format(\"parquet\")\n",
    "  .queryName(\"email_traffic_query\") # optional name\n",
    "  .trigger(processingTime=\"10 second\") # how often data is fetched from source\n",
    "  .option(\"checkpointLocation\", checkpoint_path) # used for fault-tolerance. Note: every query needs to have a unique check point location\n",
    "  .start(output_path) # location where the file will be written\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f07a81-32c1-401c-aec4-ce4051aa94bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# monitor the query\n",
    "\n",
    "#devices_query.id # unique per query, persisted when restarted from checkpoint \n",
    "#devices_query.name\n",
    "#devices_query.status # isDataAvailable = new data available, isTriggerActive = trigger actively firing\n",
    "#devices_query.awaitTermination(5) # used in non-Databricks usecases for keeping the thread (streaming query) alive. \n",
    "#devices_query.stop() # use to shut down the streaming query. Especially in Databricks, otherwise cluster will keep awake indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ffc6e36-a824-4911-a47c-9a8487cd0d02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can also write it into a delta table\n",
    "\n",
    "checkpoint_path = \"streaming/email_delta/checkpoint\" \n",
    "output_path = \"spark-warehouse/email_streaming\" # note that we set the output path to wherever the DWH catalog path is (in our case, ./spark-warehouse)\n",
    "\n",
    "(email_df.writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")\n",
    "  .queryName(\"email_delta_query\")\n",
    "  .trigger(processingTime=\"30 second\")\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .start(output_path) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default (outside of Databricks), a new table is not created in the Delta metastore. \n",
    "# use the following query to \"register\" the table:\n",
    "\n",
    "spark.sql(\"CREATE TABLE email_streaming USING DELTA LOCATION 'email_streaming'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this is now an EXTERNAL (unmanaged) table\n",
    "\n",
    "spark.sql(\"DESCRIBE EXTENDED email_streaming\")\n",
    "\n",
    "# the main difference is in how the data is handled:\n",
    "# MANAGED --> DROP TABLE --> drops the \"table\" as well as the data in it\n",
    "# EXTERNAL --> DROP TABLE --> drops the \"table\" (data remains in the path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view snapshots of table \n",
    "\n",
    "display(spark.table(\"email_streaming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also check the count of this table increasing\n",
    "\n",
    "spark.table(\"email_streaming\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it was a delta table, we can look at the history\n",
    "\n",
    "display(spark.sql(\"DESCRIBE HISTORY email_streaming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also create views on top of the data \n",
    "(spark\n",
    " .read\n",
    " .format(\"delta\")\n",
    " .load(\"spark-warehouse/email_streaming\")\n",
    " .createOrReplaceTempView(\"email_streaming_vw\")\n",
    ")\n",
    "\n",
    "display(spark.table(\"email_streaming_vw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5d3804-072b-4846-94b5-f0c99699d65a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use spark.streams.active to loop over all active streams\n",
    "# remember to stop streams if not working on them anymore\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "  stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4d9fd9-1d6b-4884-ac51-afe9c2b8b32b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* Delta Lake MERGE INTO:\n",
    "  * https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html\n",
    "* Structured Streaming: \n",
    "  * https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html\n",
    "  * https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html\n",
    "  * http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "  * https://docs.databricks.com/spark/latest/structured-streaming/production.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f980944-6bcd-4fdc-8081-f5efed6f66dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tasks\n",
    "\n",
    "##### Task 1\n",
    "Load the `employees.csv` dataset into a delta table `DimEmployee` with mapping the following columns:\n",
    "* employee_id\n",
    "* employee_name\n",
    "* department\n",
    "* region\n",
    "* employee_key\n",
    "* active_record (drop this field)\n",
    "* active_record_start (drop this field)\n",
    "* active_record_end (drop this field)\n",
    "* job_id (constant value `1` in initial load)\n",
    "* valid_from (map this to the same value as active_record_start)\n",
    "* valid_to (map this to the same value as active_record_end, if NULL then map it to 9999-12-31)\n",
    "\n",
    "You have to upsert the data from `employees2024.csv`.\n",
    "* Assume that `employee_id` is the unique value per employee.  \n",
    "* The fields `employee_name`, `department` and `region` should be checked for changes.\n",
    "* Update any data that becomes invalid by setting `valid_to` equal to yesterday's date (NB! do not hardcode yesterday's date)\n",
    "  * Remember that only currently _valid_ data should be updated\n",
    "  * Note: you may need an additional merge statement for which you can use this: https://docs.delta.io/latest/delta-update.html#modify-all-unmatched-rows-using-merge\n",
    "* Insert any new data that should be inserted, with `valid_from` set to current date (NB! do not hardcode current date)\n",
    "* For new data, you can hardcode the `valid_to` field to `9999-12-31`\n",
    "* The updates and inserts should have `job_id = 2`\n",
    "* Display the results from the table by selecting all fields `WHERE job_id = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd9cca6f-5579-4372-8a37-3dfe1ff96d2e",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2\n",
    "Create a schema and a streaming dataframe for the JSON files in the `gaming_data` dataset.\n",
    "  \n",
    "  \n",
    "Use the following as basis for creating your schema:  \n",
    " |-- eventName: string (nullable = true)  \n",
    " |-- eventParams: struct (nullable = true)  \n",
    " |    |-- amount: double (nullable = true)  \n",
    " |    |-- app_name: string (nullable = true)  \n",
    " |    |-- app_version: string (nullable = true)  \n",
    " |    |-- client_event_time: string (nullable = true)  \n",
    " |    |-- device_id: string (nullable = true)  \n",
    " |    |-- game_keyword: string (nullable = true)  \n",
    " |    |-- platform: string (nullable = true)  \n",
    " |    |-- scoreAdjustment: long (nullable = true)  \n",
    "  \n",
    "Read in 2 files per trigger.\n",
    "  \n",
    "Create a new modified dataframe:\n",
    "* keep only rows where eventName is \"scoreAdjustment\"\n",
    "* select the *game_keyword*, *platform* and *scoreAdjustment* columns from the eventParams struct.  \n",
    "* set trigger to run every 5 seconds.\n",
    "\n",
    "Write the datastream to a delta table called score_adjustments.  \n",
    "Check to make sure that the table has some data - this should also be visible in the cell results.  \n",
    "Show that the number of rows in the table is increasing.  \n",
    "Then stop the datastream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2086678445992806,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Practice session - Delta Lake, Structured Streaming",
   "notebookOrigID": 2086678445992776,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
